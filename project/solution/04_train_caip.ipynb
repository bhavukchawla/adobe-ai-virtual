{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML model on Cloud AI Platform\n",
    "\n",
    "This notebook shows how to:\n",
    "* Export training code from [a Keras notebook](../solution/03_taxifare_fc.ipynb) into a trainer file\n",
    "* Create a Docker container based on a [DLVM container](https://cloud.google.com/ai-platform/deep-learning-containers/docs/kubernetes-container)\n",
    "* Deploy training job to cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Export the data from BigQuery to GCS\n",
    "1. Navigate to [export_data.ipynb](05_export_data.ipynb)\n",
    "2. Update 'your-gcs-project-here' to your GCP project name\n",
    "3. Run all the notebook cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Edit notebook parameters\n",
    "1. Navigate to [notebook_params.yaml](notebook_params.yaml)\n",
    "2. Replace the bucket name with your own bucket containing your model (likely gcp-project with -ml at the end)\n",
    "3. Save the notebook\n",
    "4. Return to this notebook and continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export code from notebook\n",
    "\n",
    "This notebook extracts code from a notebook and creates a Python file suitable for use as model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import nbformat\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "def write_parameters(cell_source, params_yaml, outfp):\n",
    "    with open(params_yaml, 'r') as ifp:\n",
    "        y = yaml.safe_load(ifp)\n",
    "        # print out all the lines in notebook\n",
    "        write_code(cell_source, 'PARAMS from notebook', outfp)\n",
    "        # print out YAML file; this will override definitions above\n",
    "        formats = [\n",
    "            '{} = {}', # for integers and floats\n",
    "            '{} = \"{}\"', # for strings\n",
    "        ]\n",
    "        write_code(\n",
    "            '\\n'.join([\n",
    "                formats[type(value) is str].format(key, value) for key, value in y.items()]),\n",
    "            'PARAMS from YAML',\n",
    "            outfp\n",
    "        )\n",
    "\n",
    "def write_code(cell_source, comment, outfp):\n",
    "    lines = cell_source.split('\\n')\n",
    "    if len(lines) > 0 and lines[0].startswith('%%'):\n",
    "        prefix = '#'\n",
    "    else:\n",
    "        prefix = ''\n",
    "    \n",
    "    print(\"### BEGIN {} ###\".format(comment), file=outfp)\n",
    "    for line in lines:\n",
    "        line = prefix + line.replace('print(', 'logging.info(')\n",
    "        if len(line) > 0 and (line[0] == '!' or line[0] == '%'):\n",
    "            print('#' + line, file=outfp)\n",
    "        else:\n",
    "            print(line, file=outfp)\n",
    "    print(\"### END {} ###\\n\".format(comment), file=outfp)\n",
    "            \n",
    "def convert_notebook(notebook_filename, params_yaml, outfp):\n",
    "    write_code('import logging', 'code added by notebook conversion', outfp)\n",
    "    with open(INPUT) as ifp:\n",
    "        nb = nbformat.reads(ifp.read(), nbformat.NO_CONVERT)\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                if 'tags' in cell.metadata and 'display' in cell.metadata.tags:\n",
    "                    logging.info('Ignoring cell # {} with display tag'.format(cell.execution_count))\n",
    "                elif 'tags' in cell.metadata and 'parameters' in cell.metadata.tags:\n",
    "                    logging.info('Writing params cell # {}'.format(cell.execution_count))\n",
    "                    write_parameters(cell.source, PARAMS, outfp)\n",
    "                else:\n",
    "                    logging.info('Writing model cell # {}'.format(cell.execution_count))\n",
    "                    write_code(cell.source, 'Cell #{}'.format(cell.execution_count), outfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "INPUT='../solution/03_taxifare_fc.ipynb'\n",
    "PARAMS='./notebook_params.yaml'\n",
    "OUTDIR='./container/trainer'\n",
    "\n",
    "!mkdir -p $OUTDIR\n",
    "OUTFILE=os.path.join(OUTDIR, 'model.py')\n",
    "!touch $OUTDIR/__init__.py\n",
    "with open(OUTFILE, 'w') as ofp:\n",
    "    #convert_notebook(INPUT, PARAMS, sys.stdout)\n",
    "    convert_notebook(INPUT, PARAMS, ofp)\n",
    "#!cat $OUTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out model file\n",
    "\n",
    "<b>Note</b> Once the training starts, __Interrupt the Kernel__ (from the notebook ribbon bar above). Because it processes the entire dataset, this will take a long time on the relatively small machine on which you are running Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-07 11:06:14.031703: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2020-04-07 11:06:14.032286: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fb5c8c6f20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-07 11:06:14.032348: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-04-07 11:06:14.032546: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dropoff_latitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropoff_longitude (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_longitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_latitude (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_datetime (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_latitude (Lambda) (None,)              0           dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_longitude (Lambda (None,)              0           dropoff_longitude[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "euclidean (Lambda)              (None,)              0           pickup_longitude[0][0]           \n",
      "                                                                 pickup_latitude[0][0]            \n",
      "                                                                 dropoff_longitude[0][0]          \n",
      "                                                                 dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hourofday (Lambda)              (None,)              0           pickup_datetime[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "passenger_count (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_latitude (Lambda)  (None,)              0           pickup_latitude[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_longitude (Lambda) (None,)              0           pickup_longitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (None, 130)          1000000     scale_dropoff_latitude[0][0]     \n",
      "                                                                 scale_dropoff_longitude[0][0]    \n",
      "                                                                 euclidean[0][0]                  \n",
      "                                                                 hourofday[0][0]                  \n",
      "                                                                 passenger_count[0][0]            \n",
      "                                                                 scale_pickup_latitude[0][0]      \n",
      "                                                                 scale_pickup_longitude[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "h1 (Dense)                      (None, 32)           4192        dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "h2 (Dense)                      (None, 8)            264         h1[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "fare (Dense)                    (None, 1)            9           h2[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 1,004,465\n",
      "Trainable params: 1,004,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train for 31250 steps, validate for 10 steps\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 00001: saving model to gs://project02-253610-ml/quests/serverlessml//checkpoints/taxi\n",
      "31250/31250 - 458s - loss: 33.0931 - rmse: 3.8630 - mse: 33.0933 - val_loss: 102.2954 - val_rmse: 9.9660 - val_mse: 102.2954\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 00002: saving model to gs://project02-253610-ml/quests/serverlessml//checkpoints/taxi\n",
      "31250/31250 - 461s - loss: 17.8273 - rmse: 3.0526 - mse: 17.8273 - val_loss: 77.5954 - val_rmse: 8.7865 - val_mse: 77.5955\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 00003: saving model to gs://project02-253610-ml/quests/serverlessml//checkpoints/taxi\n",
      "31250/31250 - 443s - loss: 17.3193 - rmse: 3.0055 - mse: 17.3193 - val_loss: 59.6347 - val_rmse: 7.7115 - val_mse: 59.6347\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 00004: saving model to gs://project02-253610-ml/quests/serverlessml//checkpoints/taxi\n",
      "31250/31250 - 450s - loss: 17.0045 - rmse: 2.9655 - mse: 17.0045 - val_loss: 89.1694 - val_rmse: 9.4219 - val_mse: 89.1694\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 00005: saving model to gs://project02-253610-ml/quests/serverlessml//checkpoints/taxi\n",
      "31250/31250 - 447s - loss: 17.3137 - rmse: 3.0476 - mse: 17.3137 - val_loss: 40.1839 - val_rmse: 6.3126 - val_mse: 40.1839\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 31.8505 - rmse: 5.3170 - mse: 31.8505\n",
      "2020-04-07 11:43:59.191907: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "!python3 $OUTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Docker container\n",
    "\n",
    "Package up the trainer file into a Docker container and submit the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "\n",
    "#RUN python3 -m pip install --upgrade --quiet tf-nightly-2.0-preview\n",
    "RUN python3 -m pip install --upgrade --quiet cloudml-hypertune\n",
    "\n",
    "COPY trainer /trainer\n",
    "CMD [\"python3\", \"/trainer/model.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/push_docker.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/push_docker.sh\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=serverlessml_training_container\n",
    "#export IMAGE_TAG=$(date +%Y%m%d_%H%M%S)\n",
    "#export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME\n",
    "\n",
    "echo \"Building  $IMAGE_URI\"\n",
    "docker build -f Dockerfile -t $IMAGE_URI ./\n",
    "echo \"Pushing $IMAGE_URI\"\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container\n",
      "container/push_docker.sh\n",
      "container/.ipynb_checkpoints\n",
      "container/.ipynb_checkpoints/Dockerfile-checkpoint\n",
      "container/trainer\n",
      "container/trainer/__init__.py\n",
      "container/trainer/model.py\n",
      "container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "!find container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: If you get a permissions error when running push_docker.sh from Notebooks, do it from CloudShell:\n",
    "* Open [CloudShell](https://console.cloud.google.com/cloudshell) on the GCP Console\n",
    "*  ```git clone https://github.com/bhavukchawla/adobe-ai-virtual```\n",
    "*  ```cd adobe-ai-virtual/project/solution/container/```\n",
    "*  ```bash push_docker.sh```\n",
    "\n",
    "This next step takes 5 - 10 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building  gcr.io/project02-253610/serverlessml_training_container\n",
      "Sending build context to Docker daemon  19.46kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
      " ---> 4f3009408e35\n",
      "Step 2/4 : RUN python3 -m pip install --upgrade --quiet cloudml-hypertune\n",
      " ---> Using cache\n",
      " ---> e685e824ac79\n",
      "Step 3/4 : COPY trainer /trainer\n",
      " ---> Using cache\n",
      " ---> 3f1caf14d504\n",
      "Step 4/4 : CMD [\"python3\", \"/trainer/model.py\"]\n",
      " ---> Using cache\n",
      " ---> 680f88f21fa5\n",
      "Successfully built 680f88f21fa5\n",
      "Successfully tagged gcr.io/project02-253610/serverlessml_training_container:latest\n",
      "Pushing gcr.io/project02-253610/serverlessml_training_container\n",
      "The push refers to repository [gcr.io/project02-253610/serverlessml_training_container]\n",
      "954a26d988dd: Preparing\n",
      "2355fb33d07d: Preparing\n",
      "41b379bf2eb3: Preparing\n",
      "77db3bd6efb2: Preparing\n",
      "07fd8d677a40: Preparing\n",
      "992939e921d8: Preparing\n",
      "a304fb96c494: Preparing\n",
      "431f13f6088f: Preparing\n",
      "f63b09c90bb4: Preparing\n",
      "eac1f876522f: Preparing\n",
      "391d3bae9f0a: Preparing\n",
      "0d69a3a385f0: Preparing\n",
      "dab306330749: Preparing\n",
      "a6c05cf3b0f4: Preparing\n",
      "1c0e7affc630: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "431f13f6088f: Waiting\n",
      "f63b09c90bb4: Waiting\n",
      "dab306330749: Waiting\n",
      "a6c05cf3b0f4: Waiting\n",
      "eac1f876522f: Waiting\n",
      "1c0e7affc630: Waiting\n",
      "1852b2300972: Waiting\n",
      "391d3bae9f0a: Waiting\n",
      "0d69a3a385f0: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "992939e921d8: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "a304fb96c494: Waiting\n",
      "954a26d988dd: Layer already exists\n",
      "07fd8d677a40: Layer already exists\n",
      "41b379bf2eb3: Layer already exists\n",
      "2355fb33d07d: Layer already exists\n",
      "77db3bd6efb2: Layer already exists\n",
      "992939e921d8: Layer already exists\n",
      "a304fb96c494: Layer already exists\n",
      "eac1f876522f: Layer already exists\n",
      "431f13f6088f: Layer already exists\n",
      "f63b09c90bb4: Layer already exists\n",
      "391d3bae9f0a: Layer already exists\n",
      "0d69a3a385f0: Layer already exists\n",
      "a6c05cf3b0f4: Layer already exists\n",
      "dab306330749: Layer already exists\n",
      "1c0e7affc630: Layer already exists\n",
      "1852b2300972: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "latest: digest: sha256:e7d95f6229d041dc4c700ed66bf79396c591f9ed7c588fb8f316806e319b0522 size: 4294\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd container\n",
    "bash push_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to AI Platform\n",
    "\n",
    "Submit a training job using this custom container that we have just built. After you submit the job, [monitor it here](https://console.cloud.google.com/ai-platform/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: serverlessml_20200407_114508\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [serverlessml_20200407_114508] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe serverlessml_20200407_114508\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs serverlessml_20200407_114508\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JOBID=serverlessml_$(date +%Y%m%d_%H%M%S)\n",
    "REGION=us-central1\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$(gcloud config list project --format \"value(core.project)\")-ml\n",
    "\n",
    "#IMAGE=gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "IMAGE=gcr.io/$PROJECT_ID/serverlessml_training_container\n",
    "\n",
    "gcloud beta ai-platform jobs submit training $JOBID \\\n",
    "   --staging-bucket=gs://$BUCKET  --region=$REGION \\\n",
    "   --master-image-uri=$IMAGE \\\n",
    "   --master-machine-type=n1-standard-4 --scale-tier=CUSTOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job will take 35 - 45 minutes to complete on the dataset. You can cancel the job once you confirm it started and have inspected the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 DataCouch.\n",
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
